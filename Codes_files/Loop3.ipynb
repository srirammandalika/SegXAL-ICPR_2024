{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"/Users/srirammandalika/Downloads/unlabeled_data/Loop3\")\n",
    "train_dir = os.path.join(data_dir, \"/Users/srirammandalika/Downloads/unlabeled_data/Loop3/Original\") \n",
    "val_dir = os.path.join(data_dir, \"/Users/srirammandalika/Downloads/unlabeled_data/Loop3/val\")\n",
    "train_fns = os.listdir(train_dir)\n",
    "val_fns = os.listdir(val_dir)\n",
    "print(len(train_fns), len(val_fns))\n",
    "\n",
    "sample_image_fp = os.path.join(train_dir, train_fns[0])\n",
    "sample_image = Image.open(sample_image_fp).convert(\"RGB\")\n",
    "\n",
    "num_items = 2000\n",
    "color_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\n",
    "print(color_array.shape)\n",
    "print(color_array[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "label_model = KMeans(n_clusters=num_classes)\n",
    "label_model.fit(color_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        \n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
    "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
    "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
    "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
    "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
    "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
    "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
    "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
    "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels))\n",
    "        return block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
    "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
    "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
    "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
    "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
    "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
    "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
    "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
    "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
    "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
    "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
    "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
    "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
    "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
    "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
    "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
    "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
    "        return output_out\n",
    "\n",
    "    \n",
    "    \n",
    "class CityscapeDataset(Dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, image_dir, label_model):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_fns = os.listdir(image_dir)\n",
    "        self.label_model = label_model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.image_fns[index]\n",
    "        image_fp = os.path.join(self.image_dir, image_fn)\n",
    "        image = Image.open(image_fp).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        cityscape, label = self.split_image(image)\n",
    "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
    "        cityscape = self.transform(cityscape)\n",
    "        label_class = torch.Tensor(label_class).long()\n",
    "        return cityscape, label_class\n",
    "    \n",
    "    def split_image(self, image):\n",
    "        image = np.array(image)\n",
    "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
    "        return cityscape, label\n",
    "    \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "model_path = \"/Users/srirammandalika/Desktop/U-Net-v11.pth\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_ = UNet(num_classes=num_classes).to(device)\n",
    "model_.load_state_dict(torch.load(model_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78af635",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "dataset = CityscapeDataset(val_dir, label_model)\n",
    "data_loader = DataLoader(dataset, batch_size=test_batch_size)\n",
    "\n",
    "X, Y = next(iter(data_loader))\n",
    "X, Y = X.to(device), Y.to(device)\n",
    "Y_pred = model_(X)\n",
    "print(Y_pred.shape)\n",
    "Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09574bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_entropy(outputs, targets):\n",
    "    # Convert the outputs to probabilities by applying a softmax function\n",
    "    outputs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Extract the probabilities for the correct class from the output\n",
    "    correct_class_probs = outputs.gather(1, targets.unsqueeze(1))\n",
    "    \n",
    "    # Calculate the entropy for each pixel\n",
    "    entropy = -1 * correct_class_probs * torch.log(correct_class_probs + 1e-30)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "class EntropyLoss(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(EntropyLoss, self).init()\n",
    "    def forward(self, pred, target):\n",
    "    # convert target to one-hot encoding\n",
    "        target_onehot = F.one_hot(target, num_classes=self.num_classes).float()\n",
    "    # calculate the negative of element-wise product of predicted probability and log of predicted probability\n",
    "        entropy = -1 * (target_onehot * torch.log(pred+1e-30)).sum(dim=1).mean()\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "entropy_loss_fn = EntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2701645",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = UNet(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"/Users/srirammandalika/Desktop/U-Net-v11.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X.cpu())\n",
    "entropy = calculate_entropy(output,Y_pred.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366c1a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(entropy[0].shape)\n",
    "e = entropy.cpu().detach().numpy()\n",
    "print(e.shape)\n",
    "# print(e.shape)\n",
    "for i in range(16):\n",
    "    plt.imshow(e[i][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grid = torchvision.utils.make_grid(entropy[0])\n",
    "plt.imshow(e_grid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b43a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n",
    "\n",
    "for i in range(test_batch_size):\n",
    "    \n",
    "    landscape3 = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
    "    label_class3 = Y[i].cpu().detach().numpy()\n",
    "    label_class_predicted3 = Y_pred[i].cpu().detach().numpy()\n",
    "    \n",
    "    axes[i, 0].imshow(landscape3)\n",
    "    axes[i, 0].set_title(\"Landscape\")\n",
    "    axes[i, 1].imshow(label_class3)\n",
    "    axes[i, 1].set_title(\"Label Class\")\n",
    "    axes[i, 2].imshow(label_class_predicted3)\n",
    "    axes[i, 2].set_title(\"Label Class - Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b59d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def patchwise_entropy(image, patch_size=8):\n",
    "    h, w = image.shape\n",
    "    entropy_vals = []\n",
    "    for i in range(0, h - patch_size + 1, patch_size):\n",
    "        for j in range(0, w - patch_size + 1, patch_size):\n",
    "            patch = image[i:i + patch_size, j:j + patch_size]\n",
    "            hist = np.histogram(patch, bins=256, range=(0, 255))[0]\n",
    "            hist = hist / np.sum(hist)\n",
    "            patch_entropy = entropy(hist, base=2)\n",
    "            entropy_vals.append(patch_entropy)\n",
    "    return entropy_vals\n",
    "\n",
    "# Use the Y_pred[0] as it is, assuming it's already a numpy array\n",
    "image = Y_pred[0]\n",
    "\n",
    "# Calculate patchwise entropy\n",
    "entropy_vals = patchwise_entropy(image)\n",
    "\n",
    "# Create a heatmap of the entropy values\n",
    "entropy_map = np.zeros(image.shape)\n",
    "k = 0\n",
    "for i in range(0, image.shape[0] - 8 + 1, 8):\n",
    "    for j in range(0, image.shape[1] - 8 + 1, 8):\n",
    "        entropy_map[i:i + 8, j:j + 8] = entropy_vals[k]\n",
    "        k += 1\n",
    "\n",
    "# Display the entropy map\n",
    "plt.imshow(entropy_map, cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d99110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_model, num_images=1500):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_model = label_model\n",
    "        \n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise ValueError(f\"Directory does not exist: {image_dir}\")\n",
    "        \n",
    "        # Load only the first 'num_images' images\n",
    "        self.image_fns = [f for f in sorted(os.listdir(image_dir)) if f.endswith('.jpg')]\n",
    "        if not self.image_fns:\n",
    "            raise ValueError(f\"No JPG files found in directory: {image_dir}\")\n",
    "        \n",
    "        self.image_fns = self.image_fns[:num_images]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.image_fns[index]\n",
    "        image_fp = os.path.join(self.image_dir, image_fn)\n",
    "        image = Image.open(image_fp).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        cityscape, label = self.split_image(image)\n",
    "        \n",
    "        # Ensure label is not empty\n",
    "        if label.size == 0:\n",
    "            raise ValueError(f\"Label image is empty for {image_fn}\")\n",
    "        \n",
    "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(cityscape.shape[0], cityscape.shape[1])\n",
    "        cityscape = self.transform(cityscape)\n",
    "        label_class = torch.Tensor(label_class).long()\n",
    "        return cityscape, label_class\n",
    "    \n",
    "    def split_image(self, image):\n",
    "        cityscape = image[:, :256, :]\n",
    "        label = image[:, 256:, :]\n",
    "        return cityscape, label\n",
    "    \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84506bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e754dc16",
   "metadata": {},
   "source": [
    "## MiDas Depth Analyser (PAE Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4665ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.transforms import Compose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MiDaS model\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "# Choose the right model type\n",
    "model_type = \"DPT_Large\"  # Replace with \"DPT_Hybrid\" or \"MiDaS_small\" as needed\n",
    "\n",
    "# Select the appropriate transforms\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Path to your image\n",
    "filename = '/Users/srirammandalika/Downloads/unlabeled_data/Loop1/raw/0.png'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(filename):\n",
    "    print(f\"The file {filename} does not exist.\")\n",
    "else:\n",
    "    # Read and process the image\n",
    "    img = cv2.imread(filename)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply transforms\n",
    "        input_batch = transform(img).to(device)\n",
    "\n",
    "        # Perform depth estimation\n",
    "        with torch.no_grad():\n",
    "            prediction = midas(input_batch)\n",
    "\n",
    "            # Resize to original image resolution\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "\n",
    "        # Convert to numpy array and normalize\n",
    "        output = prediction.cpu().numpy()\n",
    "        output_normalized = cv2.normalize(output, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Apply colormap for visualization\n",
    "        depth_colormap = cv2.applyColorMap(output_normalized.astype('uint8'), cv2.COLORMAP_JET)\n",
    "\n",
    "        # Display depth map using matplotlib\n",
    "        plt.imshow(depth_colormap)\n",
    "        plt.title('Depth Map')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Unable to load image {filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Example: Load a pre-trained model (like ResNet)\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.feature_maps = None\n",
    "        self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.feature_maps = output\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        layer = self.model._modules.get(self.target_layer)\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_heatmap(self, input_img):\n",
    "        # Forward pass\n",
    "        output = self.model(input_img)\n",
    "\n",
    "        # You need to specify the index of the class in the output\n",
    "        # Here, we are taking the class with the highest output value\n",
    "        score = output[:, output.argmax()]\n",
    "\n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        score.backward()\n",
    "\n",
    "        # Weighted combination of feature maps\n",
    "        weighted_combination = torch.mean(self.gradients, [2, 3], keepdim=True)\n",
    "        weighted_feature_maps = F.relu((weighted_combination * self.feature_maps).sum(1))\n",
    "\n",
    "        # Normalize the heatmap\n",
    "        heatmap = weighted_feature_maps.squeeze().cpu().detach().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "        return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw image\n",
    "image_path = '/Users/srirammandalika/Downloads/unlabeled_data/Loop1/raw/0.png'  # Replace with your local path\n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the transformation for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transformation to the image\n",
    "input_tensor = transform(raw_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Initialize Grad-CAM (Assuming GradCAM class and generate_heatmap method are defined)\n",
    "grad_cam = GradCAM(model, 'layer4')\n",
    "\n",
    "# Generate the heatmap from the model\n",
    "heatmap = grad_cam.generate_heatmap(input_tensor)\n",
    "\n",
    "# Normalize and convert heatmap to colormap for visualization\n",
    "heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n",
    "heatmap = np.uint8(255 * (heatmap - heatmap_min) / (heatmap_max - heatmap_min))\n",
    "colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# Resize the heatmap to the size of the raw image\n",
    "colored_heatmap = cv2.resize(colored_heatmap, (raw_image.width, raw_image.height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Load MiDaS model for depth estimation\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Prepare the image for depth estimation\n",
    "# The transforms for the MiDaS model are designed to be applied to NumPy arrays, not PIL Images\n",
    "midas_transform = midas_transforms.dpt_transform\n",
    "\n",
    "# Convert the PIL image to a NumPy array\n",
    "img_np = np.array(raw_image)\n",
    "\n",
    "# Apply the MiDaS transforms\n",
    "input_batch = midas_transform(img_np).to(device)\n",
    "\n",
    "# Perform depth estimation\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "\n",
    "    # Resize to original image resolution\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=(raw_image.height, raw_image.width),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "# Convert to numpy array and normalize\n",
    "depth_map = prediction.cpu().numpy()\n",
    "depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Apply colormap for visualization\n",
    "depth_colormap = cv2.applyColorMap(depth_map_normalized.astype('uint8'), cv2.COLORMAP_JET)\n",
    "\n",
    "# Overlay the heatmap on the depth map\n",
    "superimposed_img = cv2.addWeighted(depth_colormap, 0.6, colored_heatmap, 0.4, 0)\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load your depth map and raw image here\n",
    "# depth_map = cv2.imread('path_to_depth_map.png', cv2.IMREAD_UNCHANGED)\n",
    "raw_image = Image.open('/Users/srirammandalika/Downloads/unlabeled_data/Loop1/raw/0.png').convert('RGB')\n",
    "\n",
    "# Normalize the depth map to have values between 0 and 1\n",
    "depth_map_normalized = cv2.normalize(depth_map, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_32F)\n",
    "\n",
    "# Convert raw_image to numpy array if it's a PIL Image\n",
    "raw_image_np = np.array(raw_image)\n",
    "\n",
    "# Ensure the raw_image is normalized between 0 and 1\n",
    "raw_image_np = raw_image_np.astype(np.float32) / 255.0\n",
    "\n",
    "# Expand the depth map to three channels by replicating the single channel across the RGB channels\n",
    "depth_map_expanded = np.repeat(depth_map_normalized[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "# Perform the pathwise product\n",
    "pathwise_product = np.multiply(depth_map_expanded, raw_image_np)\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(pathwise_product)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assume pathwise_product is the output from the previous code block and is available here\n",
    "\n",
    "# Convert pathwise_product to PIL Image to apply transformation\n",
    "pathwise_product_pil = Image.fromarray((pathwise_product * 255).astype('uint8'), 'RGB')\n",
    "\n",
    "# Define the transformation for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transformation to the pathwise product image\n",
    "input_tensor = transform(pathwise_product_pil).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Initialize Grad-CAM (Make sure to define GradCAM class or import it if it's from an external library)\n",
    "grad_cam = GradCAM(model, 'layer4')\n",
    "\n",
    "# Generate the heatmap from the model\n",
    "heatmap = grad_cam.generate_heatmap(input_tensor)\n",
    "\n",
    "# Normalize and convert heatmap to colormap for visualization\n",
    "heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n",
    "heatmap = np.uint8(255 * (heatmap - heatmap_min) / (heatmap_max - heatmap_min))\n",
    "colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# Invert the colors of the heatmap\n",
    "colored_heatmap = colored_heatmap[:, :, ::-1]\n",
    "\n",
    "# Resize the heatmap to the size of the pathwise product image\n",
    "colored_heatmap = cv2.resize(colored_heatmap, (pathwise_product_pil.width, pathwise_product_pil.height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Overlay the heatmap on the pathwise product image\n",
    "superimposed_img = cv2.addWeighted(np.array(pathwise_product_pil), 0.6, colored_heatmap, 0.4, 0)\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046254c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a6615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Assuming GradCAM class and other necessary imports are already defined\n",
    "\n",
    "# Your entropy and Grad-CAM code here\n",
    "\n",
    "# Assuming Y_pred[0] is the image for which you want to create the overlay\n",
    "image = Y_pred[0]  # Replace this with your actual image\n",
    "entropy_vals = patchwise_entropy(image)  # Calculate entropy\n",
    "\n",
    "# Create an entropy map\n",
    "entropy_map = np.zeros(image.shape)\n",
    "k = 0\n",
    "for i in range(0, image.shape[0] - 8 + 1, 8):\n",
    "    for j in range(0, image.shape[1] - 8 + 1, 8):\n",
    "        entropy_map[i:i + 8, j:j + 8] = entropy_vals[k]\n",
    "        k += 1\n",
    "\n",
    "# Convert entropy map to PIL Image for processing\n",
    "entropy_map_pil = Image.fromarray(np.uint8(entropy_map / np.max(entropy_map) * 255), 'L')\n",
    "entropy_map_pil = entropy_map_pil.resize((224, 224))  # Resize to match Grad-CAM size\n",
    "\n",
    "# Prepare the entropy map for overlay\n",
    "entropy_map_pil = transforms.functional.to_tensor(entropy_map_pil).unsqueeze(0)\n",
    "\n",
    "# Generate Grad-CAM heatmap\n",
    "# ... Grad-CAM code ...\n",
    "\n",
    "# Resize Grad-CAM heatmap to match the entropy map size\n",
    "colored_heatmap = cv2.resize(colored_heatmap, (entropy_map_pil.shape[2], entropy_map_pil.shape[3]), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Convert Grad-CAM heatmap to tensor for overlay\n",
    "colored_heatmap_tensor = transforms.functional.to_tensor(colored_heatmap).unsqueeze(0)\n",
    "\n",
    "# Overlay the entropy map and Grad-CAM heatmap\n",
    "superimposed_img = 0.6 * entropy_map_pil + 0.4 * colored_heatmap_tensor\n",
    "superimposed_img = transforms.functional.to_pil_image(superimposed_img.squeeze(0))\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5274a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa34c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f11c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c678e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf168770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ee2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_model, num_images=1200):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_model = label_model\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise ValueError(f\"Directory does not exist: {image_dir}\")\n",
    "\n",
    "        # Load only the first 'num_images' images\n",
    "        self.image_fns = [f for f in sorted(os.listdir(image_dir)) if f.endswith('.jpg')]\n",
    "        if not self.image_fns:\n",
    "            raise ValueError(f\"No PNG files found in directory: {image_dir}\")\n",
    "\n",
    "        self.image_fns = self.image_fns[:num_images]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.image_fns[index]\n",
    "        image_fp = os.path.join(self.image_dir, image_fn)\n",
    "        image = Image.open(image_fp).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        cityscape, label = self.split_image(image)\n",
    "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
    "        cityscape = self.transform(cityscape)\n",
    "        label_class = torch.Tensor(label_class).long()\n",
    "        return cityscape, label_class\n",
    "    \n",
    "    def split_image(self, image):\n",
    "        image = np.array(image)\n",
    "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
    "        return cityscape, label\n",
    "    \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce6b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce27d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83148ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68e21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed56a251",
   "metadata": {},
   "source": [
    "## DICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c89a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06511d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d0413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e20593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1db87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class RawAndLabelDataset(Dataset):\n",
    "    def __init__(self, raw_dir, label_dir, transform=None):\n",
    "        self.raw_dir = raw_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(raw_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_path = os.path.join(self.raw_dir, self.image_files[idx])\n",
    "        label_filename = self.image_files[idx].replace('.jpg', '_label.jpg')  # Adjust if label filenames differ\n",
    "        label_path = os.path.join(self.label_dir, label_filename)\n",
    "\n",
    "        raw_image = read_image(raw_path).float() / 255.0\n",
    "        label_image = read_image(label_path).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            raw_image = self.transform(raw_image)\n",
    "            label_image = self.transform(label_image)\n",
    "\n",
    "        return raw_image, label_image\n",
    "\n",
    "def calculate_patchwise_entropy(image, patch_size=8):\n",
    "    # Assuming image is a 2D numpy array (segmentation result)\n",
    "    entropy_vals = []\n",
    "    for i in range(0, image.shape[0] - patch_size + 1, patch_size):\n",
    "        for j in range(0, image.shape[1] - patch_size + 1, patch_size):\n",
    "            patch = image[i:i + patch_size, j:j + patch_size]\n",
    "            hist, _ = np.histogram(patch, bins=range(256), density=True)\n",
    "            ent = entropy(hist, base=2)\n",
    "            entropy_vals.append(ent)\n",
    "    entropy_map = np.array(entropy_vals).reshape((image.shape[0] // patch_size, image.shape[1] // patch_size))\n",
    "    return entropy_map\n",
    "\n",
    "# Load the U-Net model\n",
    "model_path = '/Users/srirammandalika/Desktop/U-Net-v11.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Set up dataset and data loader\n",
    "raw_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/unsupervised-rep'\n",
    "label_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/US_label'\n",
    "transform = transforms.Compose([transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = RawAndLabelDataset(raw_dir, label_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Visualize the images and entropy maps\n",
    "for raw_image, label_image in loader:\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(to_pil_image(raw_image.cpu().squeeze()))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(predicted, cmap='viridis')\n",
    "    plt.title('Predicted Segmentation')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(entropy_map, cmap='jet')\n",
    "    plt.title('Entropy Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2602a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the U-Net model\n",
    "# ... [rest of your model loading code] ...\n",
    "\n",
    "# Set up dataset and data loader\n",
    "# ... [rest of your dataset setup] ...\n",
    "\n",
    "# Calculating entropy\n",
    "entropy_values = []\n",
    "\n",
    "for raw_image, label_image in loader:\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "    average_entropy = np.mean(entropy_map)\n",
    "    entropy_values.append(average_entropy)\n",
    "\n",
    "# Plotting the entropy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(entropy_values)), entropy_values, '-o')\n",
    "plt.title('Average Entropy per Image')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Average Entropy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (raw_image, label_image) in enumerate(loader):\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "    average_entropy = np.mean(entropy_map)\n",
    "    entropy_values.append(average_entropy)\n",
    "\n",
    "    print(f\"Image {i}: Average Entropy = {average_entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ce8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entropy = 0\n",
    "\n",
    "for raw_image, label_image in loader:\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "    average_entropy = np.mean(entropy_map)\n",
    "    entropy_values.append(average_entropy)\n",
    "    total_entropy += average_entropy\n",
    "\n",
    "combined_average_entropy = total_entropy / len(loader)\n",
    "print(f\"Combined Average Entropy = {combined_average_entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a61c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0470e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(test_batch_size, 4, figsize=(4*5, test_batch_size*5))\n",
    "\n",
    "for i in range(test_batch_size):\n",
    "    # Assuming you have the images in the correct format\n",
    "    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
    "    label_class = Y[i].cpu().detach().numpy()\n",
    "    label_class_predicted = Y_pred[i]  # Directly use Y_pred[i] as it's already a numpy array\n",
    "\n",
    "    # Calculate entropy and create an entropy map for the current segmented image\n",
    "    entropy_vals = patchwise_entropy(label_class_predicted)\n",
    "    entropy_map = np.zeros(label_class_predicted.shape)\n",
    "    k = 0\n",
    "    for x in range(0, label_class_predicted.shape[0] - 8 + 1, 8):\n",
    "        for y in range(0, label_class_predicted.shape[1] - 8 + 1, 8):\n",
    "            entropy_map[x:x + 8, y:y + 8] = entropy_vals[k]\n",
    "            k += 1\n",
    "\n",
    "    # Define high entropy threshold\n",
    "    high_entropy_threshold = np.mean(entropy_vals) + np.std(entropy_vals)\n",
    "\n",
    "    # Create a mask for high entropy patches\n",
    "    high_entropy_mask = entropy_map > high_entropy_threshold\n",
    "\n",
    "    # Replace high entropy patches in label_class_predicted with corresponding patches from label_class\n",
    "    modified_segmented_image = np.copy(label_class_predicted)\n",
    "    for x in range(0, label_class_predicted.shape[0] - 8 + 1, 8):\n",
    "        for y in range(0, label_class_predicted.shape[1] - 8 + 1, 8):\n",
    "            if high_entropy_mask[x:x + 8, y:y + 8].any():  # Check if any part of the patch is high entropy\n",
    "                modified_segmented_image[x:x + 8, y:y + 8] = label_class[x:x + 8, y:y + 8]\n",
    "\n",
    "    # Plotting\n",
    "    axes[i, 0].imshow(landscape)\n",
    "    axes[i, 0].set_title(\"Landscape\")\n",
    "    axes[i, 1].imshow(label_class)\n",
    "    axes[i, 1].set_title(\"Label Class\")\n",
    "    axes[i, 2].imshow(label_class_predicted)\n",
    "    axes[i, 2].set_title(\"Label Class - Predicted\")\n",
    "    axes[i, 3].imshow(modified_segmented_image)\n",
    "    axes[i, 3].set_title(\"Modified Segmented Image\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
