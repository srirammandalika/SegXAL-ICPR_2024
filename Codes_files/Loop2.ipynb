{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9127eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir2 = os.path.join(\"/Users/srirammandalika/Downloads/unlabeled_data/Loop2\")\n",
    "train_dir2 = os.path.join(data_dir2, \"/Users/srirammandalika/Downloads/unlabeled_data/Loop2/actual\") \n",
    "val_dir2 = os.path.join(data_dir2, \"/Users/srirammandalika/Downloads/unlabeled_data/Loop2/val\")\n",
    "train_fns2 = os.listdir(train_dir2)\n",
    "val_fns2 = os.listdir(val_dir2)\n",
    "print(len(train_fns2), len(val_fns2))\n",
    "\n",
    "sample_image_fp2 = os.path.join(train_dir2, train_fns2[0])\n",
    "sample_image2 = Image.open(sample_image_fp2).convert(\"RGB\")\n",
    "\n",
    "num_items2 = 1213\n",
    "color_array = np.random.choice(range(256), 3*num_items2).reshape(-1, 3)\n",
    "print(color_array.shape)\n",
    "print(color_array[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68342256",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "label_model2 = KMeans(n_clusters=num_classes)\n",
    "label_model2.fit(color_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        \n",
    "        super(UNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
    "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
    "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
    "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
    "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
    "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
    "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
    "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
    "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
    "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels))\n",
    "        return block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
    "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
    "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
    "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
    "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
    "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
    "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
    "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
    "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
    "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
    "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
    "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
    "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
    "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
    "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
    "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
    "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
    "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
    "        return output_out\n",
    "\n",
    "    \n",
    "    \n",
    "class CityscapeDataset(Dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, image_dir, label_model):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_fns = os.listdir(image_dir)\n",
    "        self.label_model = label_model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.image_fns[index]\n",
    "        image_fp = os.path.join(self.image_dir, image_fn)\n",
    "        image = Image.open(image_fp).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        cityscape, label = self.split_image(image)\n",
    "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
    "        cityscape = self.transform(cityscape)\n",
    "        label_class = torch.Tensor(label_class).long()\n",
    "        return cityscape, label_class\n",
    "    \n",
    "    def split_image(self, image):\n",
    "        image = np.array(image)\n",
    "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
    "        return cityscape, label\n",
    "    \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3647146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "model_path2 = \"/Users/srirammandalika/Desktop/U-Net-v11.pth\"\n",
    "device2 = torch.device(\"cpu\")\n",
    "\n",
    "model_ = UNet(num_classes=num_classes).to(device2)\n",
    "model_.load_state_dict(torch.load(model_path2, map_location=device2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 16\n",
    "dataset2 = CityscapeDataset(val_dir2, label_model2)\n",
    "data_loader2 = DataLoader(dataset2, batch_size=test_batch_size)\n",
    "\n",
    "X, Y = next(iter(data_loader2))\n",
    "X, Y = X.to(device2), Y.to(device2)\n",
    "Y_pred = model_(X)\n",
    "print(Y_pred.shape)\n",
    "Y_pred = torch.argmax(Y_pred, dim=1)\n",
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27affbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_entropy(outputs, targets):\n",
    "    # Convert the outputs to probabilities by applying a softmax function\n",
    "    outputs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Extract the probabilities for the correct class from the output\n",
    "    correct_class_probs = outputs.gather(1, targets.unsqueeze(1))\n",
    "    \n",
    "    # Calculate the entropy for each pixel\n",
    "    entropy = -1 * correct_class_probs * torch.log(correct_class_probs + 1e-30)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "class EntropyLoss(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(EntropyLoss, self).init()\n",
    "    def forward(self, pred, target):\n",
    "    # convert target to one-hot encoding\n",
    "        target_onehot = F.one_hot(target, num_classes=self.num_classes).float()\n",
    "    # calculate the negative of element-wise product of predicted probability and log of predicted probability\n",
    "        entropy = -1 * (target_onehot * torch.log(pred+1e-30)).sum(dim=1).mean()\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "entropy_loss_fn = EntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = UNet(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"/Users/srirammandalika/Desktop/U-Net-v11.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X.cpu())\n",
    "entropy = calculate_entropy(output,Y_pred.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy[0].shape)\n",
    "e = entropy.cpu().detach().numpy()\n",
    "print(e.shape)\n",
    "# print(e.shape)\n",
    "for i in range(16):\n",
    "    plt.imshow(e[i][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_grid = torchvision.utils.make_grid(entropy[0])\n",
    "plt.imshow(e_grid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ced06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n",
    "\n",
    "for i in range(test_batch_size):\n",
    "    \n",
    "    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
    "    label_class = Y[i].cpu().detach().numpy()\n",
    "    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n",
    "    \n",
    "    axes[i, 0].imshow(landscape)\n",
    "    axes[i, 0].set_title(\"Landscape\")\n",
    "    axes[i, 1].imshow(label_class)\n",
    "    axes[i, 1].set_title(\"Label Class\")\n",
    "    axes[i, 2].imshow(label_class_predicted)\n",
    "    axes[i, 2].set_title(\"Label Class - Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c402c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def patchwise_entropy(image, patch_size=8):\n",
    "    h, w = image.shape\n",
    "    entropy_vals = []\n",
    "    for i in range(0, h - patch_size + 1, patch_size):\n",
    "        for j in range(0, w - patch_size + 1, patch_size):\n",
    "            patch = image[i:i + patch_size, j:j + patch_size]\n",
    "            hist = np.histogram(patch, bins=256, range=(0, 255))[0]\n",
    "            hist = hist / np.sum(hist)\n",
    "            patch_entropy = entropy(hist, base=2)\n",
    "            entropy_vals.append(patch_entropy)\n",
    "    return entropy_vals\n",
    "\n",
    "# Assuming Y_pred is a numpy array with shape (batch_size, H, W)\n",
    "batch_size = Y_pred.shape[0]\n",
    "\n",
    "for i in range(batch_size):\n",
    "    image = Y_pred[i]\n",
    "\n",
    "    # Calculate patchwise entropy\n",
    "    entropy_vals = patchwise_entropy(image)\n",
    "\n",
    "    # Create a heatmap of the entropy values\n",
    "    entropy_map = np.zeros(image.shape)\n",
    "    k = 0\n",
    "    for x in range(0, image.shape[0] - 8 + 1, 8):\n",
    "        for y in range(0, image.shape[1] - 8 + 1, 8):\n",
    "            entropy_map[x:x + 8, y:y + 8] = entropy_vals[k]\n",
    "            k += 1\n",
    "\n",
    "    # Display the entropy map\n",
    "    plt.figure(figsize=(5, 5))  # Adjust figure size as needed\n",
    "    plt.imshow(entropy_map, cmap='jet')\n",
    "    plt.title(f'Entropy Map for Image {i + 1}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc63a82f",
   "metadata": {},
   "source": [
    "## Unsupervised Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e073f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming UNet is defined and matches the architecture of the saved state dict\n",
    "# Also assuming num_classes is defined\n",
    "\n",
    "# Load the U-Net model\n",
    "model_path = '/Users/srirammandalika/Desktop/U-Net-v1.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define a simple dataset just to load images\n",
    "class RawImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = read_image(img_path).float() / 255.0  # Convert image to float and scale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "# Setup dataset and data loader\n",
    "raw_data_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/unsupervised-rep'\n",
    "raw_dataset = RawImageDataset(raw_data_dir, transform=transform)\n",
    "raw_loader = DataLoader(raw_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Perform semantic segmentation and plot the images\n",
    "for images in raw_loader:\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    predicted_labels = torch.argmax(outputs, dim=1).cpu()\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(17, 4))\n",
    "\n",
    "        # Original Image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Original Image\")\n",
    "        # Undo normalization\n",
    "        original_image = images[i].cpu().clone()\n",
    "        original_image = original_image * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        original_image = to_pil_image(original_image)\n",
    "        plt.imshow(original_image)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predicted Segmentation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Predicted Segmentation\")\n",
    "        predicted_segmentation = predicted_labels[i]\n",
    "        plt.imshow(predicted_segmentation, cmap='viridis')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80c4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, raw_dir, label_dir, transform=None):\n",
    "        self.raw_dir = raw_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(raw_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_path = os.path.join(self.raw_dir, self.image_files[idx])\n",
    "        label_filename = self.image_files[idx].replace('.jpg', '_label.jpg')\n",
    "        label_path = os.path.join(self.label_dir, label_filename)\n",
    "\n",
    "        raw_image = read_image(raw_path).float() / 255.0\n",
    "        label_image = read_image(label_path).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            raw_image = self.transform(raw_image)\n",
    "            label_image = self.transform(label_image)\n",
    "\n",
    "        return raw_image, label_image\n",
    "\n",
    "# Instantiate the dataset and DataLoader\n",
    "raw_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/unsupervised-rep'\n",
    "label_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/US_label'\n",
    "dataset = SegmentationDataset(raw_dir, label_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Process the dataset\n",
    "# ...\n",
    "for i, (raw_images, label_images) in enumerate(dataloader):\n",
    "    raw_images = raw_images.to(device)\n",
    "    label_images = label_images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_masks = model(raw_images)\n",
    "\n",
    "    for j in range(raw_images.size(0)):\n",
    "        single_raw_image = raw_images[j].permute(1, 2, 0).cpu().numpy()\n",
    "        single_label_image = label_images[j].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Assuming the model returns a 4D tensor (B, C, H, W)\n",
    "        # Convert predicted_masks to a 2D array by selecting the class with the highest probability\n",
    "        single_predicted_mask = torch.argmax(predicted_masks[j], dim=0).cpu().numpy()\n",
    "\n",
    "        entropy_map = calculate_entropy(single_predicted_mask)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "        axes[0].imshow(single_raw_image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[1].imshow(single_label_image, cmap='viridis')\n",
    "        axes[1].set_title('Ground Truth Label')\n",
    "        axes[2].imshow(single_predicted_mask, cmap='viridis')\n",
    "        axes[2].set_title('Predicted Segmentation')\n",
    "        entropy_im = axes[3].imshow(entropy_map, cmap='jet')\n",
    "        axes[3].set_title('Entropy Map')\n",
    "        plt.show()\n",
    "\n",
    "    if i == 0:  # Remove this condition to process all images\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972a5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ebf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1612d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1848d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "class RawAndLabelDataset(Dataset):\n",
    "    def __init__(self, raw_dir, label_dir, transform=None):\n",
    "        self.raw_dir = raw_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(raw_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_path = os.path.join(self.raw_dir, self.image_files[idx])\n",
    "        label_filename = self.image_files[idx].replace('.jpg', '_label.jpg')  # Adjust if label filenames differ\n",
    "        label_path = os.path.join(self.label_dir, label_filename)\n",
    "\n",
    "        raw_image = read_image(raw_path).float() / 255.0\n",
    "        label_image = read_image(label_path).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            raw_image = self.transform(raw_image)\n",
    "            label_image = self.transform(label_image)\n",
    "\n",
    "        return raw_image, label_image\n",
    "\n",
    "def calculate_patchwise_entropy(image, patch_size=8):\n",
    "    # Assuming image is a 2D numpy array (segmentation result)\n",
    "    entropy_vals = []\n",
    "    for i in range(0, image.shape[0] - patch_size + 1, patch_size):\n",
    "        for j in range(0, image.shape[1] - patch_size + 1, patch_size):\n",
    "            patch = image[i:i + patch_size, j:j + patch_size]\n",
    "            hist, _ = np.histogram(patch, bins=range(256), density=True)\n",
    "            ent = entropy(hist, base=2)\n",
    "            entropy_vals.append(ent)\n",
    "    entropy_map = np.array(entropy_vals).reshape((image.shape[0] // patch_size, image.shape[1] // patch_size))\n",
    "    return entropy_map\n",
    "\n",
    "# Load the U-Net model\n",
    "model_path = '/Users/srirammandalika/Desktop/U-Net-v1.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Set up dataset and data loader\n",
    "raw_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/unsupervised-rep'\n",
    "label_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/US_label'\n",
    "transform = transforms.Compose([transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = RawAndLabelDataset(raw_dir, label_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Visualize the images and entropy maps\n",
    "for raw_image, label_image in loader:\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(to_pil_image(raw_image.cpu().squeeze()))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(predicted, cmap='viridis')\n",
    "    plt.title('Predicted Segmentation')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(entropy_map, cmap='jet')\n",
    "    plt.title('Entropy Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8941c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86400474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the U-Net model\n",
    "# ... [rest of your model loading code] ...\n",
    "\n",
    "# Set up dataset and data loader\n",
    "# ... [rest of your dataset setup] ...\n",
    "\n",
    "# Calculating entropy\n",
    "entropy_values = []\n",
    "\n",
    "for raw_image, label_image in loader:\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "    average_entropy = np.mean(entropy_map)\n",
    "    entropy_values.append(average_entropy)\n",
    "\n",
    "# Plotting the entropy graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(entropy_values)), entropy_values, '-o')\n",
    "plt.title('Average Entropy per Image')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Average Entropy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c92076",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (raw_image, label_image) in enumerate(loader):\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "    average_entropy = np.mean(entropy_map)\n",
    "    entropy_values.append(average_entropy)\n",
    "\n",
    "    print(f\"Image {i}: Average Entropy = {average_entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50960877",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entropy = 0\n",
    "\n",
    "for raw_image, label_image in loader:\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted = torch.argmax(output, dim=1).cpu().numpy()[0]  # Assuming batch size is 1\n",
    "\n",
    "    entropy_map = calculate_patchwise_entropy(predicted)\n",
    "    average_entropy = np.mean(entropy_map)\n",
    "    entropy_values.append(average_entropy)\n",
    "    total_entropy += average_entropy\n",
    "\n",
    "combined_average_entropy = total_entropy / len(loader)\n",
    "print(f\"Combined Average Entropy = {combined_average_entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcb5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b1ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725a87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae01f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c548b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "\n",
    "class RawAndLabelDataset(Dataset):\n",
    "    def __init__(self, raw_dir, label_dir, transform=None):\n",
    "        self.raw_dir = raw_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(raw_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_path = os.path.join(self.raw_dir, self.image_files[idx])\n",
    "        label_filename = self.image_files[idx].replace('.jpg', '_label.jpg')\n",
    "        label_path = os.path.join(self.label_dir, label_filename)\n",
    "\n",
    "        raw_image = read_image(raw_path).float() / 255.0\n",
    "        label_image = read_image(label_path).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            raw_image = self.transform(raw_image)\n",
    "            label_image = self.transform(label_image)\n",
    "\n",
    "        return raw_image, label_image\n",
    "\n",
    "def iou_score(pred, target):\n",
    "    # Convert predictions and target from probabilities to binary values\n",
    "    pred = np.round(pred)\n",
    "    target = np.round(target)\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = np.logical_and(pred, target)\n",
    "    union = np.logical_or(pred, target)\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou\n",
    "\n",
    "# Load the U-Net model\n",
    "model_path = '/Users/srirammandalika/Desktop/U-Net-v11.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "raw_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/unsupervised-rep'\n",
    "label_dir = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/US_label'\n",
    "dataset = RawAndLabelDataset(raw_dir, label_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "for i, (raw_image, label_image) in enumerate(dataloader):\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted_mask = torch.argmax(output, dim=1)\n",
    "\n",
    "    predicted_mask_np = predicted_mask.cpu().numpy().squeeze()\n",
    "    label_image_np = label_image.cpu().numpy().squeeze()\n",
    "\n",
    "    # Calculate and print IoU for each image\n",
    "    iou = iou_score(predicted_mask_np, label_image_np)\n",
    "    print(f\"Image {i}: IoU = {iou:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ed1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Load the model, dataset, and dataloader as before\n",
    "# ...\n",
    "\n",
    "# Create a figure for plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))  # Adjust subplot layout as needed\n",
    "axes = axes.flatten()\n",
    "plot_index = 0\n",
    "\n",
    "# Process each image in the dataset\n",
    "for i, (raw_image, label_image) in enumerate(dataloader):\n",
    "    raw_image = raw_image.to(device)\n",
    "    label_image = label_image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(raw_image)\n",
    "        predicted_mask = torch.argmax(output, dim=1)\n",
    "\n",
    "    predicted_mask_np = predicted_mask.cpu().numpy().squeeze()\n",
    "    label_image_np = label_image.cpu().numpy().squeeze()\n",
    "\n",
    "    # Calculate IoU for each image\n",
    "    iou = iou_score(predicted_mask_np, label_image_np)\n",
    "\n",
    "    # Plot images with IoU greater than 0.60\n",
    "    if iou > 0.6:\n",
    "        print(f\"Image {i}: IoU = {iou:.2f}\")\n",
    "        raw_img = to_pil_image(raw_image.squeeze().cpu())\n",
    "        pred_img = to_pil_image(predicted_mask.squeeze().cpu().byte())  # Convert to byte tensor\n",
    "\n",
    "        if plot_index < 4:  # Adjust this based on the number of subplots\n",
    "            axes[plot_index].imshow(raw_img)\n",
    "            axes[plot_index].set_title(f'Original Image {i}')\n",
    "            axes[plot_index + 1].imshow(pred_img, cmap='viridis')\n",
    "            axes[plot_index + 1].set_title(f'Predicted Mask {i}')\n",
    "            plot_index += 2\n",
    "\n",
    "        if plot_index >= 4:  # Adjust based on the number of subplots\n",
    "            pass\n",
    "\n",
    "plt.suptitle('Labeled Pool')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b4d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d3763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8323e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887864cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064ad41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f6518cb",
   "metadata": {},
   "source": [
    "## MiDas (PAE Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.transforms import Compose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MiDaS model\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "# Choose the right model type\n",
    "model_type = \"DPT_Large\"  # Replace with \"DPT_Hybrid\" or \"MiDaS_small\" as needed\n",
    "\n",
    "# Select the appropriate transforms\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Path to your image\n",
    "filename = '/Users/srirammandalika/Downloads/unlabeled_data/Loop2/unsupervised-rep/8.jpg'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(filename):\n",
    "    print(f\"The file {filename} does not exist.\")\n",
    "else:\n",
    "    # Read and process the image\n",
    "    img = cv2.imread(filename)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply transforms\n",
    "        input_batch = transform(img).to(device)\n",
    "\n",
    "        # Perform depth estimation\n",
    "        with torch.no_grad():\n",
    "            prediction = midas(input_batch)\n",
    "\n",
    "            # Resize to original image resolution\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "\n",
    "        # Convert to numpy array and normalize\n",
    "        output = prediction.cpu().numpy()\n",
    "        output_normalized = cv2.normalize(output, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Apply colormap for visualization\n",
    "        depth_colormap = cv2.applyColorMap(output_normalized.astype('uint8'), cv2.COLORMAP_JET)\n",
    "\n",
    "        # Display depth map using matplotlib\n",
    "        plt.imshow(depth_colormap)\n",
    "        plt.title('Depth Map')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Unable to load image {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Example: Load a pre-trained model (like ResNet)\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08adc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.feature_maps = None\n",
    "        self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.feature_maps = output\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "\n",
    "        layer = self.model._modules.get(self.target_layer)\n",
    "        layer.register_forward_hook(forward_hook)\n",
    "        layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_heatmap(self, input_img):\n",
    "        # Forward pass\n",
    "        output = self.model(input_img)\n",
    "\n",
    "        # You need to specify the index of the class in the output\n",
    "        # Here, we are taking the class with the highest output value\n",
    "        score = output[:, output.argmax()]\n",
    "\n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        score.backward()\n",
    "\n",
    "        # Weighted combination of feature maps\n",
    "        weighted_combination = torch.mean(self.gradients, [2, 3], keepdim=True)\n",
    "        weighted_feature_maps = F.relu((weighted_combination * self.feature_maps).sum(1))\n",
    "\n",
    "        # Normalize the heatmap\n",
    "        heatmap = weighted_feature_maps.squeeze().cpu().detach().numpy()\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "        return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e10c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw image\n",
    "image_path = '/Users/srirammandalika/Downloads/unlabeled_data/Loop1/raw/10.png'  # Replace with your local path\n",
    "raw_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the transformation for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transformation to the image\n",
    "input_tensor = transform(raw_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Initialize Grad-CAM (Assuming GradCAM class and generate_heatmap method are defined)\n",
    "grad_cam = GradCAM(model, 'layer4')\n",
    "\n",
    "# Generate the heatmap from the model\n",
    "heatmap = grad_cam.generate_heatmap(input_tensor)\n",
    "\n",
    "# Normalize and convert heatmap to colormap for visualization\n",
    "heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n",
    "heatmap = np.uint8(255 * (heatmap - heatmap_min) / (heatmap_max - heatmap_min))\n",
    "colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# Resize the heatmap to the size of the raw image\n",
    "colored_heatmap = cv2.resize(colored_heatmap, (raw_image.width, raw_image.height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Load MiDaS model for depth estimation\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS\")\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Prepare the image for depth estimation\n",
    "# The transforms for the MiDaS model are designed to be applied to NumPy arrays, not PIL Images\n",
    "midas_transform = midas_transforms.dpt_transform\n",
    "\n",
    "# Convert the PIL image to a NumPy array\n",
    "img_np = np.array(raw_image)\n",
    "\n",
    "# Apply the MiDaS transforms\n",
    "input_batch = midas_transform(img_np).to(device)\n",
    "\n",
    "# Perform depth estimation\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "\n",
    "    # Resize to original image resolution\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=(raw_image.height, raw_image.width),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "# Convert to numpy array and normalize\n",
    "depth_map = prediction.cpu().numpy()\n",
    "depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Apply colormap for visualization\n",
    "depth_colormap = cv2.applyColorMap(depth_map_normalized.astype('uint8'), cv2.COLORMAP_JET)\n",
    "\n",
    "# Overlay the heatmap on the depth map\n",
    "superimposed_img = cv2.addWeighted(depth_colormap, 0.6, colored_heatmap, 0.4, 0)\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load your depth map and raw image here\n",
    "# depth_map = cv2.imread('path_to_depth_map.png', cv2.IMREAD_UNCHANGED)\n",
    "raw_image = Image.open('/Users/srirammandalika/Downloads/unlabeled_data/Loop1/raw/10.png').convert('RGB')\n",
    "\n",
    "# Normalize the depth map to have values between 0 and 1\n",
    "depth_map_normalized = cv2.normalize(depth_map, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_32F)\n",
    "\n",
    "# Convert raw_image to numpy array if it's a PIL Image\n",
    "raw_image_np = np.array(raw_image)\n",
    "\n",
    "# Ensure the raw_image is normalized between 0 and 1\n",
    "raw_image_np = raw_image_np.astype(np.float32) / 255.0\n",
    "\n",
    "# Expand the depth map to three channels by replicating the single channel across the RGB channels\n",
    "depth_map_expanded = np.repeat(depth_map_normalized[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "# Perform the pathwise product\n",
    "pathwise_product = np.multiply(depth_map_expanded, raw_image_np)\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(pathwise_product)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assume pathwise_product is the output from the previous code block and is available here\n",
    "\n",
    "# Convert pathwise_product to PIL Image to apply transformation\n",
    "pathwise_product_pil = Image.fromarray((pathwise_product * 255).astype('uint8'), 'RGB')\n",
    "\n",
    "# Define the transformation for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transformation to the pathwise product image\n",
    "input_tensor = transform(pathwise_product_pil).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Initialize Grad-CAM (Make sure to define GradCAM class or import it if it's from an external library)\n",
    "grad_cam = GradCAM(model, 'layer4')\n",
    "\n",
    "# Generate the heatmap from the model\n",
    "heatmap = grad_cam.generate_heatmap(input_tensor)\n",
    "\n",
    "# Normalize and convert heatmap to colormap for visualization\n",
    "heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n",
    "heatmap = np.uint8(255 * (heatmap - heatmap_min) / (heatmap_max - heatmap_min))\n",
    "colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# Invert the colors of the heatmap\n",
    "colored_heatmap = colored_heatmap[:, :, ::-1]\n",
    "\n",
    "# Resize the heatmap to the size of the pathwise product image\n",
    "colored_heatmap = cv2.resize(colored_heatmap, (pathwise_product_pil.width, pathwise_product_pil.height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Overlay the heatmap on the pathwise product image\n",
    "superimposed_img = cv2.addWeighted(np.array(pathwise_product_pil), 0.6, colored_heatmap, 0.4, 0)\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cd486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090fc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0c4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce11562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29a8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ad5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Assuming GradCAM class and other necessary imports are already defined\n",
    "\n",
    "# Your entropy and Grad-CAM code here\n",
    "\n",
    "# Assuming Y_pred[0] is the image for which you want to create the overlay\n",
    "image = Y_pred[0]  # Replace this with your actual image\n",
    "entropy_vals = patchwise_entropy(image)  # Calculate entropy\n",
    "\n",
    "# Create an entropy map\n",
    "entropy_map = np.zeros(image.shape)\n",
    "k = 0\n",
    "for i in range(0, image.shape[0] - 8 + 1, 8):\n",
    "    for j in range(0, image.shape[1] - 8 + 1, 8):\n",
    "        entropy_map[i:i + 8, j:j + 8] = entropy_vals[k]\n",
    "        k += 1\n",
    "\n",
    "# Convert entropy map to PIL Image for processing\n",
    "entropy_map_pil = Image.fromarray(np.uint8(entropy_map / np.max(entropy_map) * 255), 'L')\n",
    "entropy_map_pil = entropy_map_pil.resize((224, 224))  # Resize to match Grad-CAM size\n",
    "\n",
    "# Prepare the entropy map for overlay\n",
    "entropy_map_pil = transforms.functional.to_tensor(entropy_map_pil).unsqueeze(0)\n",
    "\n",
    "# Generate Grad-CAM heatmap\n",
    "# ... Grad-CAM code ...\n",
    "\n",
    "# Resize Grad-CAM heatmap to match the entropy map size\n",
    "colored_heatmap = cv2.resize(colored_heatmap, (entropy_map_pil.shape[2], entropy_map_pil.shape[3]), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Convert Grad-CAM heatmap to tensor for overlay\n",
    "colored_heatmap_tensor = transforms.functional.to_tensor(colored_heatmap).unsqueeze(0)\n",
    "\n",
    "# Overlay the entropy map and Grad-CAM heatmap\n",
    "superimposed_img = 0.6 * entropy_map_pil + 0.4 * colored_heatmap_tensor\n",
    "superimposed_img = transforms.functional.to_pil_image(superimposed_img.squeeze(0))\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47818c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635beb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97382603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(test_batch_size, 4, figsize=(4*5, test_batch_size*5))\n",
    "\n",
    "for i in range(test_batch_size):\n",
    "    # Assuming you have the images in the correct format\n",
    "    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
    "    label_class = Y[i].cpu().detach().numpy()\n",
    "    label_class_predicted = Y_pred[i]  # Directly use Y_pred[i] as it's already a numpy array\n",
    "\n",
    "    # Calculate entropy and create an entropy map for the current segmented image\n",
    "    entropy_vals = patchwise_entropy(label_class_predicted)\n",
    "    entropy_map = np.zeros(label_class_predicted.shape)\n",
    "    k = 0\n",
    "    for x in range(0, label_class_predicted.shape[0] - 8 + 1, 8):\n",
    "        for y in range(0, label_class_predicted.shape[1] - 8 + 1, 8):\n",
    "            entropy_map[x:x + 8, y:y + 8] = entropy_vals[k]\n",
    "            k += 1\n",
    "\n",
    "    # Define high entropy threshold\n",
    "    high_entropy_threshold = np.mean(entropy_vals) + np.std(entropy_vals)\n",
    "\n",
    "    # Create a mask for high entropy patches\n",
    "    high_entropy_mask = entropy_map > high_entropy_threshold\n",
    "\n",
    "    # Replace high entropy patches in label_class_predicted with corresponding patches from label_class\n",
    "    modified_segmented_image = np.copy(label_class_predicted)\n",
    "    for x in range(0, label_class_predicted.shape[0] - 8 + 1, 8):\n",
    "        for y in range(0, label_class_predicted.shape[1] - 8 + 1, 8):\n",
    "            if high_entropy_mask[x:x + 8, y:y + 8].any():  # Check if any part of the patch is high entropy\n",
    "                modified_segmented_image[x:x + 8, y:y + 8] = label_class[x:x + 8, y:y + 8]\n",
    "\n",
    "    # Plotting\n",
    "    axes[i, 0].imshow(landscape)\n",
    "    axes[i, 0].set_title(\"Landscape\")\n",
    "    axes[i, 1].imshow(label_class)\n",
    "    axes[i, 1].set_title(\"Label Class\")\n",
    "    axes[i, 2].imshow(label_class_predicted)\n",
    "    axes[i, 2].set_title(\"Label Class - Predicted\")\n",
    "    axes[i, 3].imshow(modified_segmented_image)\n",
    "    axes[i, 3].set_title(\"Modified Segmented Image\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fb10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f417b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cee298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd204de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7d91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24cfb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb9abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfe301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dafbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2edb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bb335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_model, num_images=1200):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_model = label_model\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise ValueError(f\"Directory does not exist: {image_dir}\")\n",
    "\n",
    "        # Load only the first 'num_images' images\n",
    "        self.image_fns = [f for f in sorted(os.listdir(image_dir)) if f.endswith('.jpg')]\n",
    "        if not self.image_fns:\n",
    "            raise ValueError(f\"No PNG files found in directory: {image_dir}\")\n",
    "\n",
    "        self.image_fns = self.image_fns[:num_images]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_fn = self.image_fns[index]\n",
    "        image_fp = os.path.join(self.image_dir, image_fn)\n",
    "        image = Image.open(image_fp).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        cityscape, label = self.split_image(image)\n",
    "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
    "        cityscape = self.transform(cityscape)\n",
    "        label_class = torch.Tensor(label_class).long()\n",
    "        return cityscape, label_class\n",
    "    \n",
    "    def split_image(self, image):\n",
    "        image = np.array(image)\n",
    "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
    "        return cityscape, label\n",
    "    \n",
    "    def transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset with the specified number of images\n",
    "train_dataset = CityscapeDataset(train_dir, label_model, num_images=1200)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# Test data loader\n",
    "try:\n",
    "    data, targets = next(iter(train_loader))\n",
    "    print(\"Data loader working correctly.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with data loader: {e}\")\n",
    "\n",
    "# Initialize the model\n",
    "model = UNet(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25  # Number of epochs to train for\n",
    "batches_per_epoch = 5  # Number of batches to train per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_count = 0\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count >= batches_per_epoch:\n",
    "            break\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Save the updated model\n",
    "new_model_path = '/Users/srirammandalika/Desktop/U-Net-v11.pth'\n",
    "torch.save(model.state_dict(), new_model_path)\n",
    "print(\"Updated model saved to U-Net-v11.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fbafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029811c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34b4abf",
   "metadata": {},
   "source": [
    "## IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf71bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_iou(label, prediction):\n",
    "    # Flatten the arrays to simplify calculation\n",
    "    label = label.flatten()\n",
    "    prediction = prediction.flatten()\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = np.logical_and(label, prediction)\n",
    "    union = np.logical_or(label, prediction)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    \n",
    "    return iou_score\n",
    "\n",
    "# Assuming test_batch_size, label_class, and label_class_predicted are defined as before\n",
    "iou_scores = []\n",
    "for i in range(test_batch_size):\n",
    "    # Assuming label_class and label_class_predicted are numpy arrays with the same shape\n",
    "    iou_score = calculate_iou(label_class[i], label_class_predicted[i])\n",
    "    iou_scores.append(iou_score)\n",
    "\n",
    "# Now you have the IoU scores for each image in the batch\n",
    "average_iou = np.mean(iou_scores)\n",
    "print(f\"Average IoU across the batch: {average_iou * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Production",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
